{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html \n",
    "<img src=\"/ProductReviewsProcessingRepo/images/hudi_demo_diagram.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851eda8ac5f1458e9fd1b55d930acb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SaveMode\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.types.IntegerType\n",
      "import org.apache.hudi.DataSourceWriteOptions\n",
      "import org.apache.hudi.DataSourceReadOptions\n",
      "import org.apache.hudi.config.HoodieWriteConfig\n",
      "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
      "hudiTableName: String = amazon_product_reviews\n",
      "hudiTableRecordKey: String = review_id\n",
      "hudiTablePath: String = s3://hocanint-reinvent-2019-demo-outputs/createdatasets/amazon_product_reviews\n",
      "hudiTablePartitionColumn: String = review_date\n",
      "hudiTablePrecombineKey: String = timestamp\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.DataSourceReadOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
    "\n",
    "//Specify common DataSourceWriteOptions in the single hudiOptions variable \n",
    "val hudiTableName = \"amazon_product_reviews\"\n",
    "val hudiTableRecordKey = \"review_id\"\n",
    "val hudiTablePath = \"s3://hocanint-reinvent-2019-demo-outputs/createdatasets/\" + hudiTableName\n",
    "val hudiTablePartitionColumn = \"review_date\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0718cb3637bc4dbc90fc9ab048811407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourceData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [marketplace: string, customer_id: string ... 14 more fields]\n"
     ]
    }
   ],
   "source": [
    "/****************************\n",
    "Read out product reviews table\n",
    "*****************************/\n",
    "val sourceData = spark.read.option(\"sep\", \"\\t\").option(\"header\", \"true\").parquet(\"s3://hocanint-reinvent-2019-datasets-us-east-1/parquet/product_category=Home_Improvement/*\").withColumn(hudiTablePrecombineKey, current_timestamp()).withColumn(hudiTablePartitionColumn, regexp_replace(col(hudiTablePartitionColumn), \"-\", \"/\")).withColumn(\"year\", $\"review_date\".substr(1,4)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd181b3636c4e5098ec545be6ed77a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------+--------------------+-----------+-----------+\n",
      "|marketplace|     review_id|customer_id|       product_title|star_rating|review_date|\n",
      "+-----------+--------------+-----------+--------------------+-----------+-----------+\n",
      "|         US|R2XKAHVTHMN0YZ|   10127812|RoomMates RMK2361...|          5| 2015/01/01|\n",
      "|         US|R23U5RQ70LNFES|   18462986|Progress Lighting...|          5| 2015/01/01|\n",
      "|         US| R3ZRQAED0WWBF|    9973935|Elmer's E7010 Car...|          5| 2015/01/01|\n",
      "|         US|R36EL6ASTQXWCJ|   15871971|Honeywell RCWL300...|          5| 2015/01/01|\n",
      "|         US|R33S6FX3917UAT|   53047377|Master Lock Pytho...|          5| 2015/01/01|\n",
      "|         US| RE1ZOXQ9PAHUN|   11082374|Step 2 541200 Mai...|          5| 2015/01/01|\n",
      "|         US|R1SLAY20OP3Z26|    1386285|Science Purchase ...|          5| 2015/01/01|\n",
      "|         US| R85LC7REDGM52|   10675345|Axis 45504 3 Outl...|          5| 2015/01/01|\n",
      "|         US|R1RQ9YTHRX3NZI|   15700318|General Electric ...|          4| 2015/01/01|\n",
      "|         US|R3IA977YN0DRR0|   47613773|6 Channel Fm Wire...|          5| 2015/01/01|\n",
      "+-----------+--------------+-----------+--------------------+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sourceData.select(\"marketplace\", \"review_id\", \"customer_id\", \"product_title\", \"star_rating\", \"review_date\").show(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091aad3da12640608c3a5bff22190b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.datasource.write.precombine.field -> timestamp, hoodie.datasource.hive_sync.partition_fields -> year,month,day, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.datasource.hive_sync.table -> amazon_product_reviews, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> review_id, hoodie.table.name -> amazon_product_reviews, hoodie.datasource.write.storage.type -> COPY_ON_WRITE, hoodie.datasource.hive_sync.assume_date_partitioning -> false, hoodie.datasource.write.partitionpath.field -> review_date)\n"
     ]
    }
   ],
   "source": [
    "/****************************\n",
    "Our Hudi Options for our Product Reviews Dataset.\n",
    "*****************************/\n",
    "val hudiOptions = Map[String,String](\n",
    "  HoodieWriteConfig.TABLE_NAME -> hudiTableName,\n",
    "\n",
    "  //For this data set, we will configure it to use the COPY_ON_WRITE storage strategy. You can also choose MERGE_ON_READ\n",
    "  DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY -> \"COPY_ON_WRITE\", \n",
    "\n",
    "  //These three options configure what Hudi should use as its record key, partition column, and precombine key.\n",
    "  DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"review_id\",\n",
    "  DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"timestamp\",\n",
    "  DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"review_date\",\n",
    "\n",
    "  //For this data set, we specify that we want to sync metadata with Hive. \n",
    "  DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\",\n",
    "  DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> hudiTableName,\n",
    "  DataSourceWriteOptions.HIVE_ASSUME_DATE_PARTITION_OPT_KEY -> \"false\",\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"year,month,day\",\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY → classOf[MultiPartKeysValueExtractor].getName\n",
    ")\n",
    "\n",
    "/** ********************************\n",
    "Lets write our input dataset to Hudi.\n",
    "************************************/\n",
    "(sourceData.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "\n",
    "  //Operation Key tells Hudi whether this is an Insert, Upsert, or Bulk Insert operation.\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL)\n",
    "  \n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870e1359043c4110b4aab64060b4a9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n"
     ]
    }
   ],
   "source": [
    "/** **********************************\n",
    "Lets look at a product that some of my consumers may be having an issue with.\n",
    "**************************************/\n",
    "val readOptimizedHudiViewDF = (spark.read\n",
    "       .format(\"org.apache.hudi\")\n",
    "       .load(hudiTablePath + \"/*/*/*/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed933aa75a734c2cbee5fa0ea5231bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "+-----------+--------+\n",
      "|star_rating|count(1)|\n",
      "+-----------+--------+\n",
      "|          1|   77100|\n",
      "|          2|   38288|\n",
      "|          3|   58836|\n",
      "|          4|  130377|\n",
      "|          5|  599625|\n",
      "|        100|       5|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** ***********************************\n",
    "Lets take a look at our data. Lets say someone says there is something fishy going on with star ratings.\n",
    "**************************************/\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_raw_ro_table\");\n",
    "spark.sql(\"select star_rating, count(*) from amazon_product_reviews_raw_ro_table group by star_rating order by star_rating ASC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8393e3fe8e6341fe9000dae29a08f4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsertdf: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n"
     ]
    }
   ],
   "source": [
    "/** *********************************\n",
    "Select the rows we want to update and and make the update.\n",
    "************************************/\n",
    "val upsertdf = readOptimizedHudiViewDF.filter($\"star_rating\" === 100).withColumn(\"star_rating\", lit(null).cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccaa329dbc62439fbcba6b67a2705720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/** ********************************\n",
    "Before, if you wanted to update data in S3, you had to read the old data, merge with the new data, and then overwrite\n",
    "the old data. Now, with Hudi, you can directly update the data in-place.\n",
    "************************************/\n",
    "(upsertdf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e689e5a166f64db185c99ee3c5e32cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "+-----------+--------+\n",
      "|star_rating|count(1)|\n",
      "+-----------+--------+\n",
      "|       null|       5|\n",
      "|          1|   77100|\n",
      "|          2|   38288|\n",
      "|          3|   58836|\n",
      "|          4|  130377|\n",
      "|          5|  599625|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val readOptimizedHudiViewDF = spark.read.format(\"org.apache.hudi\").load(hudiTablePath + \"/*/*/*/*\").cache()\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_ro_table\");\n",
    "spark.sql(\"select star_rating, count(*) from amazon_product_reviews_ro_table group by star_rating order by star_rating ASC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8569348fc78847f0ada5791ba4a3c4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleteRowsDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n"
     ]
    }
   ],
   "source": [
    "/** *******************************\n",
    "Now, suppose we need to delete a customers information due to GDPR because a request was made by a customer?\n",
    "***********************************/\n",
    "val deleteRowsDf = readOptimizedHudiViewDF.filter($\"customer_id\" === 32068341);\n",
    "\n",
    "//Deletion\n",
    "(deleteRowsDf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "  //We set the operation to UPSERT\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  //We set the Payload Class to be empty record \n",
    "  .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \"org.apache.hudi.EmptyHoodieRecordPayload\")\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873814a862564e3f8985c4bfd72621f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/**************************************\n",
    "We can also do point in time queries. Lets take a look at all the commits.\n",
    "***************************************/\n",
    "val readOptimizedHudiViewDF = spark.read.format(\"org.apache.hudi\").load(hudiTablePath + \"/*/*/*/*\").cache()\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_ro_table\");\n",
    "\n",
    "spark.sql(\"select count(*) from amazon_product_reviews_ro_table where customer_id = 32068341\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val commits = (spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from amazon_product_reviews_ro_table order by commitTime\")\n",
    "                .map(k => k.getString(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** ********************************\n",
    "Suppose that we wanted to know what was a review at a certain point of time. Hudi Allows that by specifying \n",
    "a point in time and it will read \n",
    "************************************/\n",
    "val beginTime = \"0\"\n",
    "val endTime = commits(0) // commit time we are interested in\n",
    "\n",
    "val amazon_product_reviews_table = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     //Mark that we want to do an incremental query\n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "\n",
    "     //Set at what time we want to start quering.\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime)\n",
    "     .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)\n",
    "\n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)).cache()\n",
    "\n",
    "amazon_product_reviews_table.select(\"review_id\", \"product_id\", \"product_title\", \"star_rating\").filter($\"star_rating\" === 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/***********************************\n",
    "At this point, I am going to switch to SQLDeveloper and call Hive Queries to show that the data has been changed\n",
    "************************************/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
