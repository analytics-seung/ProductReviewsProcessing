{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"/ProductReviewsProcessingRepo/images/hudi_demo_diagram.png\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html \n",
    "<img src=\"/ProductReviewsProcessingRepo/images/hudi_demo_diagram.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7586734c237416bb5aafe330156331b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiTableName: String = amazon_product_reviews\n",
      "hudiTableRecordKey: String = review_id\n",
      "hudiTablePrecombineKey: String = timestamp\n",
      "hudiTablePath: String = s3://hocanint-reinvent-2019-demo-outputs/createdatasets/amazon_product_reviews\n",
      "hudiTablePartitionColumn: String = review_date\n",
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.datasource.write.precombine.field -> timestamp, hoodie.datasource.hive_sync.enable -> false, hoodie.datasource.write.recordkey.field -> review_id, hoodie.table.name -> amazon_product_reviews, hoodie.datasource.write.storage.type -> MERGE_ON_READ, hoodie.datasource.write.partitionpath.field -> review_date)\n"
     ]
    }
   ],
   "source": [
    "//Specify common DataSourceWriteOptions in the single hudiOptions variable \n",
    "val hudiTableName = \"amazon_product_reviews\"\n",
    "val hudiTableRecordKey = \"review_id\"\n",
    "val hudiTablePrecombineKey = \"timestamp\"\n",
    "val hudiTablePath = \"s3://hocanint-reinvent-2019-demo-outputs/createdatasets/\" + hudiTableName\n",
    "val hudiTablePartitionColumn = \"review_date\"\n",
    "\n",
    "val hudiOptions = Map[String,String](\n",
    "  HoodieWriteConfig.TABLE_NAME -> hudiTableName,\n",
    "  //For this data set, we will configure it to use the Merge on Read storage strategy.\n",
    "  DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY -> \"MERGE_ON_READ\", \n",
    "  DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"review_id\",\n",
    "  DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> hudiTablePrecombineKey,\n",
    "  DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> hudiTablePartitionColumn,\n",
    "  DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"false\"\n",
    "  //DataSourceWriteOptions.HIVE_ASSUME_DATE_PARTITION_OPT_KEY -> \"false\",\n",
    "  //DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> hudiTableName\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9a0a985eef4d63a38c10f192d1b77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [marketplace: string, customer_id: string ... 13 more fields]\n"
     ]
    }
   ],
   "source": [
    "/****************************\n",
    "Read out product reviews table\n",
    "*****************************/\n",
    "val df = spark.read.option(\"sep\", \"\\t\").option(\"header\", \"true\").csv(\"s3://amazon-reviews-pds/tsv/amazon_reviews_us_Home_Improvement_v1_00.tsv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd42d6cca7e4e57b499de85dd4fba5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputdf: org.apache.spark.sql.DataFrame = [marketplace: string, customer_id: string ... 14 more fields]\n"
     ]
    }
   ],
   "source": [
    "/****************\n",
    "We need to add a timestamp of the current transaction and format the date column which will be used as a partition column. \n",
    "*****************/\n",
    "val inputdf = df.withColumn(hudiTablePrecombineKey, current_timestamp()).withColumn(hudiTablePartitionColumn, regexp_replace(col(hudiTablePartitionColumn), \"-\", \"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a476bab945ed47329639570ee80dd90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e022e7aeef174a5da2b0e900fd6e45ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/** ********************************\n",
    "Lets write our input dataset to Hudi.\n",
    "************************************/\n",
    "(inputdf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL)\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** **********************************\n",
    "Lets look at a product that some of my consumers may be having an issue with.\n",
    "**************************************/\n",
    "inputdf.registerTempTable(\"amazon_product_reviews_raw_ro_table\");\n",
    "spark.sql(\"select star_rating, count(*) from amazon_product_reviews_raw_ro_table group by star_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6322a4ce72674e9a925a039512a443c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsertdf: org.apache.spark.sql.DataFrame = [marketplace: string, customer_id: string ... 14 more fields]\n"
     ]
    }
   ],
   "source": [
    "/** *********************************\n",
    "Select the rows we want to update and and make the update.\n",
    "************************************/\n",
    "val upsertdf = inputdf.filter($\"customer_id\" === 17767084).withColumn(\"star_rating\", expr(\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390886453c494b1f8d8213ea12f2cf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 4 times, most recent failure: Lost task 0.3 in stage 31.0 (TID 24833, ip-172-31-22-141.ec2.internal, executor 11): org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:264)\n",
      "\tat org.apache.hudi.HoodieWriteClient.lambda$upsertRecordsInternal$507693af$1(HoodieWriteClient.java:428)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:202)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:178)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:257)\n",
      "\t... 30 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:142)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:200)\n",
      "\t... 32 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:140)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:230)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:209)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:51)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:260)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:37)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:120)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file s3://hocanint-reinvent-2019-demo-outputs/createdatasets/amazon_product_reviews/2015/08/31/17c7ee7b-1215-454a-b998-580ef21edb49-0_3924-5-3929_20191122153323.parquet\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:132)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:136)\n",
      "\tat org.apache.hudi.func.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:47)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:44)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:91)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "Caused by: java.lang.UnsupportedOperationException: org.apache.parquet.avro.AvroConverters$FieldIntegerConverter\n",
      "\tat org.apache.parquet.io.api.PrimitiveConverter.addBinary(PrimitiveConverter.java:70)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl$2$6.writeValue(ColumnReaderImpl.java:317)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:367)\n",
      "\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:226)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "  at scala.Option.foreach(Option.scala:257)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "  at org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n",
      "  at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:145)\n",
      "  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:91)\n",
      "  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "  ... 52 elided\n",
      "Caused by: org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\n",
      "  at org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:264)\n",
      "  at org.apache.hudi.HoodieWriteClient.lambda$upsertRecordsInternal$507693af$1(HoodieWriteClient.java:428)\n",
      "  at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "  at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n",
      "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "  ... 3 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "  at org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:202)\n",
      "  at org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:178)\n",
      "  at org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:257)\n",
      "  ... 30 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "  at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:142)\n",
      "  at org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:200)\n",
      "  ... 32 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "  at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "  at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "  at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:140)\n",
      "  ... 33 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "  at org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:230)\n",
      "  at org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:209)\n",
      "  at org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:51)\n",
      "  at org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:260)\n",
      "  at org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:37)\n",
      "  at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:120)\n",
      "  at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "  ... 3 more\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file s3://hocanint-reinvent-2019-demo-outputs/createdatasets/amazon_product_reviews/2015/08/31/17c7ee7b-1215-454a-b998-580ef21edb49-0_3924-5-3929_20191122153323.parquet\n",
      "  at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)\n",
      "  at org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:132)\n",
      "  at org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:136)\n",
      "  at org.apache.hudi.func.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:47)\n",
      "  at org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:44)\n",
      "  at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:91)\n",
      "  at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "  ... 4 more\n",
      "Caused by: java.lang.UnsupportedOperationException: org.apache.parquet.avro.AvroConverters$FieldIntegerConverter\n",
      "  at org.apache.parquet.io.api.PrimitiveConverter.addBinary(PrimitiveConverter.java:70)\n",
      "  at org.apache.parquet.column.impl.ColumnReaderImpl$2$6.writeValue(ColumnReaderImpl.java:317)\n",
      "  at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:367)\n",
      "  at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\n",
      "  at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:226)\n",
      "  ... 11 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** ********************************\n",
    "Before, if you wanted to update data in S3, you had to read the old data, merge with the new data, and then overwrite\n",
    "the old data. Now, with Hudi, you can directly update the data in-place.\n",
    "************************************/\n",
    "(upsertdf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  .options(hudiOptions)\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** ********************************\n",
    "Suppose that we wanted to know what was a review at a certain point of time. Hudi Allows that by specifying \n",
    "a point in time and it will read \n",
    "************************************/\n",
    "val readFromTime = date_add(current_timestamp(), -1)\n",
    "(val amazon_product_reviews_table = spark.read()\n",
    "     .format(\"org.apache.hudi\")\n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY(),\n",
    "             DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL())\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY(), readFromTime)\n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath))\n",
    "\n",
    "amazon_product_reviews_table.filter($\"customer_id\" === 17767084).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*******************************\n",
    "Hudi also provides a Read Optimized table. \n",
    "********************************/\n",
    "val roViewDF = spark.read.format(\"org.apache.hudi\").load(hudiTablePath + \"/*/*/*/*\")\n",
    "roViewDF.registerTempTable(\"amazon_product_reviews_ro_table\")\n",
    "\n",
    "spark.sql(\"select review_id, product_title, star_rating from amazon_product_reviews_ro_table where customer_id = '17767084'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** *******************************\n",
    "Now, suppose we need to delete a customers information due to GDPR because a request was made by a customer?\n",
    "***********************************/\n",
    "val deleteRowsDf = spark.read.option(\"sep\", \"\\t\").option(\"header\", \"true\")\n",
    "        .csv(\"s3://amazon-reviews-pds/tsv/amazon_reviews_us_Home_Improvement_v1_00.tsv.gz\")\n",
    "        .filter($\"customer_id\" === 17767084);\n",
    "\n",
    "//Deletion\n",
    "deleteRowsDf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \"org.apache.hudi.EmptyHoodieRecordPayload\")\n",
    "  .options(hudiOptions)\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/***********************************\n",
    "At this point, I am going to switch to SQLDeveloper and call Hive Queries to show that the data has been changed\n",
    "************************************/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
