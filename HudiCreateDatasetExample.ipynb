{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"/ProductReviewsProcessingRepo/images/hudi_demo_diagram.png\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html \n",
    "<img src=\"/ProductReviewsProcessingRepo/images/hudi_demo_diagram.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b0c25943334d0ab13bee7ccf1bb702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SaveMode\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.types.IntegerType\n",
      "import org.apache.hudi.DataSourceWriteOptions\n",
      "import org.apache.hudi.DataSourceReadOptions\n",
      "import org.apache.hudi.config.HoodieWriteConfig\n",
      "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.DataSourceReadOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c2e92d3e2446f89f4bc420cbc89245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiTableName: String = amazon_product_reviews\n",
      "hudiTableRecordKey: String = review_id\n",
      "hudiTablePrecombineKey: String = timestamp\n",
      "hudiTablePath: String = s3://hocanint-reinvent-2019-demo-outputs/createdatasets/amazon_product_reviews\n",
      "hudiTablePartitionColumn: String = review_date\n",
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.datasource.write.precombine.field -> timestamp, hoodie.datasource.hive_sync.partition_fields -> year,month,day, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.datasource.hive_sync.table -> amazon_product_reviews, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> review_id, hoodie.table.name -> amazon_product_reviews, hoodie.datasource.write.storage.type -> COPY_ON_WRITE, hoodie.datasource.hive_sync.assume_date_partitioning -> false, hoodie.datasource.write.partitionpath.field -> review_date)\n"
     ]
    }
   ],
   "source": [
    "//Specify common DataSourceWriteOptions in the single hudiOptions variable \n",
    "val hudiTableName = \"amazon_product_reviews\"\n",
    "val hudiTableRecordKey = \"review_id\"\n",
    "val hudiTablePrecombineKey = \"timestamp\"\n",
    "val hudiTablePath = \"s3://hocanint-reinvent-2019-demo-outputs/createdatasets/\" + hudiTableName\n",
    "val hudiTablePartitionColumn = \"review_date\"\n",
    "\n",
    "/****************************\n",
    "Our Hudi Options for our Product Reviews Dataset.\n",
    "*****************************/\n",
    "val hudiOptions = Map[String,String](\n",
    "  HoodieWriteConfig.TABLE_NAME -> hudiTableName,\n",
    "  //For this data set, we will configure it to use the Merge on Read storage strategy.\n",
    "  DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY -> \"COPY_ON_WRITE\", \n",
    "\n",
    "  //These three options configure what Hudi should use as its record key, partition column, and precombine key.\n",
    "  DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"review_id\",\n",
    "  DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> hudiTablePartitionColumn,\n",
    "  DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> hudiTablePrecombineKey,\n",
    "\n",
    "  //For this data set, we specify that we want to sync metadata with Hive. \n",
    "  DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\",\n",
    "  DataSourceWriteOptions.HIVE_ASSUME_DATE_PARTITION_OPT_KEY -> \"false\",\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"year,month,day\",\n",
    "  DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> hudiTableName,\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY → classOf[MultiPartKeysValueExtractor].getName\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6535fd0145c425380fac12e1a3e29ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [marketplace: string, customer_id: string ... 13 more fields]\n"
     ]
    }
   ],
   "source": [
    "/****************************\n",
    "Read out product reviews table\n",
    "*****************************/\n",
    "val df = spark.read.option(\"sep\", \"\\t\").option(\"header\", \"true\").parquet(\"s3://hocanint-reinvent-2019-datasets-us-east-1/parquet/product_category=Home_Improvement/*\").withColumn(hudiTablePrecombineKey, current_timestamp()).withColumn(hudiTablePartitionColumn, regexp_replace(col(hudiTablePartitionColumn), \"-\", \"/\")).withColumn(\"year\", $\"review_date\".substr(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc85aedafb14469a77d8f65619d071a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/** ********************************\n",
    "Lets write our input dataset to Hudi.\n",
    "************************************/\n",
    "(inputdf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "\n",
    "  //Storage type determines the storage strategy. This can be COPY_ON_WRITE or MERGE_ON_READ\n",
    "  .option(DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY, \"COPY_ON_WRITE\")\n",
    "\n",
    "  .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY, hudiTablePrecombineKey)\n",
    "\n",
    "  //Operation Key tells Hudi whether this is an Insert, Upsert, or Bulk Insert operation.\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL)\n",
    "  \n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b62e04be944faa88d7ef3b30ee99c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "+-----------+--------+\n",
      "|star_rating|count(1)|\n",
      "+-----------+--------+\n",
      "|       null|       5|\n",
      "|          1|   77100|\n",
      "|          2|   38288|\n",
      "|          3|   58836|\n",
      "|          4|  130377|\n",
      "|          5|  599625|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** **********************************\n",
    "Lets look at a product that some of my consumers may be having an issue with.\n",
    "**************************************/\n",
    "val readOptimizedHudiViewDF = spark.read.format(\"org.apache.hudi\").load(hudiTablePath + \"/*/*/*/*\")\n",
    "\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_raw_ro_table\");\n",
    "spark.sql(\"select star_rating, count(*) from amazon_product_reviews_raw_ro_table group by star_rating order by star_rating ASC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9bd245b21744569a0aea3e5bdfe1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsertdf: org.apache.spark.sql.DataFrame = [marketplace: string, customer_id: string ... 14 more fields]\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+----+--------------------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|year|           timestamp|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+----+--------------------+\n",
      "|         US|   42351637| RAWOJ1FFIPWGH|B001G0MAZ6|     343846017|Honeywell RPWL301...|       null|            0|          0|   N|                Y|             Lovely!|Bought 3 of them,...| 2015/03/11|2015|2019-11-25 00:53:...|\n",
      "|         US|   42351637|R2LJHK4EC3ZBYZ|B001G0MAVA|     484534508|Honeywell Premium...|       null|            0|          0|   N|                Y|         Love love!!|         Love Love!!| 2015/03/11|2015|2019-11-25 00:53:...|\n",
      "|         US|   42351637|R21L9NH66PU3HP|B00I0ZGOZM|     351302829|APEC Top Tier 5-S...|       null|            0|          0|   N|                Y|Wonderful and Awe...|After a year usin...| 2015/03/11|2015|2019-11-25 00:53:...|\n",
      "|         US|   42351637|R3Q1OES5YKHF2J|B0042G2W72|     683184181|Kingston Brass Te...|       null|            0|          0|   N|                Y|              SUPER!|After a year, sti...| 2015/03/11|2015|2019-11-25 00:53:...|\n",
      "|         US|   42351637|R39AOJ60FZ1OKE|B008HVH502|     486632491|Mail Boss 7506 Ma...|       null|            9|         10|   N|                Y|LOVE IT - see my ...|I gave 4 stars - ...| 2015/07/11|2015|2019-11-25 00:53:...|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** *********************************\n",
    "Select the rows we want to update and and make the update.\n",
    "************************************/\n",
    "val upsertdf = inputdf.filter($\"star_rating\" === 100).withColumn(\"star_rating\", lit(null).cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e7b61537b742779db66df812475349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/** ********************************\n",
    "Before, if you wanted to update data in S3, you had to read the old data, merge with the new data, and then overwrite\n",
    "the old data. Now, with Hudi, you can directly update the data in-place.\n",
    "************************************/\n",
    "(upsertdf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  .options(hudiOptions)\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea13a43e8354a7cb0fd62a37ad74ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "+-----------+--------+\n",
      "|star_rating|count(1)|\n",
      "+-----------+--------+\n",
      "|       null|       5|\n",
      "|          1|   77100|\n",
      "|          2|   38288|\n",
      "|          3|   58836|\n",
      "|          4|  130377|\n",
      "|          5|  599625|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val readOptimizedHudiViewDF = spark.read.format(\"org.apache.hudi\").load(hudiTablePath + \"/*/*/*/*\").cache()\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_ro_table\");\n",
    "spark.sql(\"select star_rating, count(*) from amazon_product_reviews_ro_table group by star_rating order by star_rating ASC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71e65f9244b4afc94aca40d19925130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleteRowsDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "+---------+-----------+\n",
      "|review_id|review_date|\n",
      "+---------+-----------+\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** *******************************\n",
    "Now, suppose we need to delete a customers information due to GDPR because a request was made by a customer?\n",
    "***********************************/\n",
    "val deleteRowsDf = readOptimizedHudiViewDF.filter($\"customer_id\" === 32068341);\n",
    "\n",
    "//Deletion\n",
    "(deleteRowsDf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  //We set the operation to UPSERT\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  //We set the Payload Class to be empty record \n",
    "  .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \"org.apache.hudi.EmptyHoodieRecordPayload\")\n",
    "  .options(hudiOptions)\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bf98e3d59245b5a9dc01e1345d5aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "commits: Array[String] = Array(20191125002253, 20191125005019)\n"
     ]
    }
   ],
   "source": [
    "/**************************************\n",
    "We can also do point in time queries. Lets take a look at all the commits.\n",
    "***************************************/\n",
    "val readOptimizedHudiViewDF = spark.read.format(\"org.apache.hudi\").load(hudiTablePath + \"/*/*/*/*\").cache()\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_ro_table\");\n",
    "val commits = spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from amazon_product_reviews_ro_table order by commitTime\").map(k => k.getString(0)).take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ef67d1d33d44d08c862328d3bfa8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginTime: String = 0\n",
      "endTime: String = 20191125002253\n",
      "amazon_product_reviews_table: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "+--------------+----------+--------------------+-----------+\n",
      "|     review_id|product_id|       product_title|star_rating|\n",
      "+--------------+----------+--------------------+-----------+\n",
      "| RAWOJ1FFIPWGH|B001G0MAZ6|Honeywell RPWL301...|        100|\n",
      "|R21L9NH66PU3HP|B00I0ZGOZM|APEC Top Tier 5-S...|        100|\n",
      "|R39AOJ60FZ1OKE|B008HVH502|Mail Boss 7506 Ma...|        100|\n",
      "|R2LJHK4EC3ZBYZ|B001G0MAVA|Honeywell Premium...|        100|\n",
      "|R3Q1OES5YKHF2J|B0042G2W72|Kingston Brass Te...|        100|\n",
      "+--------------+----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** ********************************\n",
    "Suppose that we wanted to know what was a review at a certain point of time. Hudi Allows that by specifying \n",
    "a point in time and it will read \n",
    "************************************/\n",
    "val beginTime = \"0\"\n",
    "val endTime = commits(0) // commit time we are interested in\n",
    "\n",
    "val amazon_product_reviews_table = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     //Mark that we want to do an incremental query\n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "\n",
    "     //Set at what time we want to start quering.\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime)\n",
    "     .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)\n",
    "\n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)).cache()\n",
    "\n",
    "amazon_product_reviews_table.select(\"review_id\", \"product_id\", \"product_title\", \"star_rating\").filter($\"star_rating\" === 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/***********************************\n",
    "At this point, I am going to switch to SQLDeveloper and call Hive Queries to show that the data has been changed\n",
    "************************************/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
