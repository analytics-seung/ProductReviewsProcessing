{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.DataSourceReadOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
    "\n",
    "//Specify common DataSourceWriteOptions in the single hudiOptions variable \n",
    "val hudiTableName = \"amazon_product_reviews\"\n",
    "val hudiTableRecordKey = \"review_id\"\n",
    "val hudiTablePath = \"s3://hocanint-reinvent-2019-demo-outputs/createdatasets/\" + hudiTableName\n",
    "val hudiTablePartitionColumn = \"review_date\"\n",
    "val hudiTablePrecombineKey = \"timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/****************************\n",
    "Read out product reviews table\n",
    "*****************************/\n",
    "val sourceData = (spark.read.parquet(\"s3://hocanint-reinvent-2019-datasets-us-east-1/parquet/product_category=Home_Improvement/*\")\n",
    "                            .withColumn(hudiTablePrecombineKey, current_timestamp().cast(\"long\"))\n",
    "                            .withColumn(hudiTablePartitionColumn, regexp_replace(col(hudiTablePartitionColumn), \"-\", \"/\"))\n",
    "                            .withColumn(\"year\", $\"review_date\".substr(1,4))\n",
    "                            .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceData.select(\"marketplace\", \"review_id\", \"customer_id\", \"product_title\", \"star_rating\", \"review_date\").show(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/****************************\n",
    "Our Hudi Options for our Product Reviews Dataset.\n",
    "*****************************/\n",
    "val hudiOptions = Map[String,String](\n",
    "  HoodieWriteConfig.TABLE_NAME -> hudiTableName,\n",
    "\n",
    "  //For this data set, we will configure it to use the COPY_ON_WRITE storage strategy. You can also choose MERGE_ON_READ\n",
    "  DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY -> \"COPY_ON_WRITE\", \n",
    "\n",
    "  //These three options configure what Hudi should use as its record key, partition column, and precombine key.\n",
    "  DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"review_id\",\n",
    "  DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"timestamp\",\n",
    "  DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"review_date\",\n",
    "\n",
    "  //For this data set, we specify that we want to sync metadata with Hive. \n",
    "  DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\",\n",
    "  DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> hudiTableName,\n",
    "  DataSourceWriteOptions.HIVE_ASSUME_DATE_PARTITION_OPT_KEY -> \"false\",\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"year,month,day\",\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY â†’ classOf[MultiPartKeysValueExtractor].getName\n",
    ")\n",
    "\n",
    "/** ********************************\n",
    "Lets write our input dataset to Hudi.\n",
    "************************************/\n",
    "(sourceData.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "\n",
    "  //Operation Key tells Hudi whether this is an Insert, Upsert, or Bulk Insert operation.\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL)\n",
    "  \n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** **********************************\n",
    "Lets look at a product that some of my consumers may be having an issue with.\n",
    "**************************************/\n",
    "val readOptimizedHudiViewDF = (spark.read\n",
    "       .format(\"org.apache.hudi\")\n",
    "       .load(hudiTablePath + \"/*/*/*/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** ***********************************\n",
    "Lets take a look at our data. Lets say someone says there is something fishy going on with star ratings.\n",
    "**************************************/\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_raw_ro_table\");\n",
    "spark.sql(\"select star_rating, count(*) from amazon_product_reviews_raw_ro_table group by star_rating order by star_rating ASC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** *********************************\n",
    "Select the rows we want to update and and make the update.\n",
    "************************************/\n",
    "val upsertdf = readOptimizedHudiViewDF.filter($\"star_rating\" === 100).withColumn(\"star_rating\", lit(null).cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** ********************************\n",
    "Before, if you wanted to update data in S3, you had to read the old data, merge with the new data, and then overwrite\n",
    "the old data. Now, with Hudi, you can directly update the data in-place.\n",
    "************************************/\n",
    "(upsertdf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val readOptimizedHudiViewDF = spark.read.format(\"org.apache.hudi\").load(hudiTablePath + \"/*/*/*/*\").cache()\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_ro_table\");\n",
    "spark.sql(\"select star_rating, count(*) from amazon_product_reviews_ro_table group by star_rating order by star_rating ASC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** *******************************\n",
    "Now, suppose we need to delete a customers information due to GDPR because a request was made by a customer?\n",
    "***********************************/\n",
    "val deleteRowsDf = readOptimizedHudiViewDF.filter($\"customer_id\" === 32068341);\n",
    "\n",
    "//Deletion\n",
    "(deleteRowsDf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "  //We set the operation to UPSERT\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  //We set the Payload Class to be empty record \n",
    "  .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \"org.apache.hudi.EmptyHoodieRecordPayload\")\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**************************************\n",
    "We can also do point in time queries. Lets take a look at all the commits.\n",
    "***************************************/\n",
    "val readOptimizedHudiViewDF = spark.read.format(\"org.apache.hudi\").load(hudiTablePath + \"/*/*/*/*\").cache()\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_ro_table\");\n",
    "\n",
    "spark.sql(\"select count(*) from amazon_product_reviews_ro_table where customer_id = 32068341\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val commits = (spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from amazon_product_reviews_ro_table order by commitTime\")\n",
    "                .map(k => k.getString(0)).take(50))\n",
    "commits.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** ********************************\n",
    "Suppose that we wanted to know what was a review at a certain point of time. Hudi Allows that by specifying \n",
    "a point in time and it will read \n",
    "************************************/\n",
    "val beginTime = \"0\"\n",
    "val endTime = commits(0) // commit time we are interested in\n",
    "\n",
    "val amazon_product_reviews_table = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     //Mark that we want to do an incremental query\n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "\n",
    "     //Set at what time we want to start quering.\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime)\n",
    "     .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)\n",
    "\n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)).cache()\n",
    "\n",
    "amazon_product_reviews_table.select(\"review_id\", \"product_id\", \"product_title\", \"star_rating\").filter($\"star_rating\" === 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/***********************************\n",
    "Hive and Presto can query the data too!\n",
    "************************************/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
