{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2214baf4992a4c9599ddbbd1bd52b245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>29</td><td>application_1574379476244_0030</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SaveMode\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.types.IntegerType\n",
      "import org.apache.hudi.DataSourceWriteOptions\n",
      "import org.apache.hudi.DataSourceReadOptions\n",
      "import org.apache.hudi.config.HoodieWriteConfig\n",
      "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
      "hudiTableName: String = amazon_product_reviews\n",
      "hudiTableRecordKey: String = review_id\n",
      "hudiTablePath: String = s3://hocanint-reinvent-2019-demo-outputs/createdatasets/amazon_product_reviews\n",
      "hudiTablePartitionColumn: String = review_date\n",
      "hudiTablePrecombineKey: String = timestamp\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.DataSourceReadOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
    "\n",
    "//Specify common DataSourceWriteOptions in the single hudiOptions variable \n",
    "val hudiTableName = \"amazon_product_reviews\"\n",
    "val hudiTableRecordKey = \"review_id\"\n",
    "val hudiTablePath = \"s3://hocanint-reinvent-2019-demo-outputs/createdatasets/\" + hudiTableName\n",
    "val hudiTablePartitionColumn = \"review_date\"\n",
    "val hudiTablePrecombineKey = \"timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79d89b323ba44f5905c9ccdf70587cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourceData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [marketplace: string, customer_id: string ... 14 more fields]\n",
      "+-----------+--------------+-----------+--------------------+-----------+-----------+\n",
      "|marketplace|     review_id|customer_id|       product_title|star_rating|review_date|\n",
      "+-----------+--------------+-----------+--------------------+-----------+-----------+\n",
      "|         US|R2XKAHVTHMN0YZ|   10127812|RoomMates RMK2361...|          5| 2015/01/01|\n",
      "|         US|R23U5RQ70LNFES|   18462986|Progress Lighting...|          5| 2015/01/01|\n",
      "|         US| R3ZRQAED0WWBF|    9973935|Elmer's E7010 Car...|          5| 2015/01/01|\n",
      "|         US|R36EL6ASTQXWCJ|   15871971|Honeywell RCWL300...|          5| 2015/01/01|\n",
      "|         US|R33S6FX3917UAT|   53047377|Master Lock Pytho...|          5| 2015/01/01|\n",
      "|         US| RE1ZOXQ9PAHUN|   11082374|Step 2 541200 Mai...|          5| 2015/01/01|\n",
      "|         US|R1SLAY20OP3Z26|    1386285|Science Purchase ...|          5| 2015/01/01|\n",
      "|         US| R85LC7REDGM52|   10675345|Axis 45504 3 Outl...|          5| 2015/01/01|\n",
      "|         US|R1RQ9YTHRX3NZI|   15700318|General Electric ...|          4| 2015/01/01|\n",
      "|         US|R3IA977YN0DRR0|   47613773|6 Channel Fm Wire...|          5| 2015/01/01|\n",
      "+-----------+--------------+-----------+--------------------+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/****************************\n",
    "Read out Amazon product reviews table\n",
    "*****************************/\n",
    "val sourceData = (spark.read.parquet(\"s3://hocanint-reinvent-2019-datasets-us-east-1/parquet/product_category=Home_Improvement/*\")\n",
    "                            .withColumn(hudiTablePrecombineKey, current_timestamp().cast(\"long\"))\n",
    "                            .withColumn(hudiTablePartitionColumn, regexp_replace(col(hudiTablePartitionColumn), \"-\", \"/\"))\n",
    "                            .cache())\n",
    "\n",
    "sourceData.select(\"marketplace\", \"review_id\", \"customer_id\", \"product_title\", \n",
    "                  \"star_rating\", \"review_date\").show(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3eb1576305e489ea82e835e52a0c274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.datasource.write.precombine.field -> timestamp, hoodie.datasource.hive_sync.partition_fields -> year,month,day, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.datasource.hive_sync.table -> amazon_product_reviews, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> review_id, hoodie.table.name -> amazon_product_reviews, hoodie.datasource.write.storage.type -> COPY_ON_WRITE, hoodie.datasource.hive_sync.assume_date_partitioning -> false, hoodie.datasource.write.partitionpath.field -> review_date)\n"
     ]
    }
   ],
   "source": [
    "/****************************\n",
    "Our Hudi Options for our Product Reviews Dataset.\n",
    "*****************************/\n",
    "val hudiOptions = Map[String,String](\n",
    "  HoodieWriteConfig.TABLE_NAME -> hudiTableName,\n",
    "\n",
    "  //For this data set, we will configure it to use the COPY_ON_WRITE storage strategy. \n",
    "  //You can also choose MERGE_ON_READ\n",
    "  DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY -> \"COPY_ON_WRITE\", \n",
    "\n",
    "  //These three options configure what Hudi should use as its record key, \n",
    "  //partition column, and precombine key.\n",
    "  DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"review_id\",\n",
    "  DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"timestamp\",\n",
    "  DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"review_date\",\n",
    "\n",
    "  //For this data set, we specify that we want to sync metadata with Hive. \n",
    "  DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\",\n",
    "  DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> hudiTableName,\n",
    "  DataSourceWriteOptions.HIVE_ASSUME_DATE_PARTITION_OPT_KEY -> \"false\",\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"year,month,day\",\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY ->\n",
    "                                        classOf[MultiPartKeysValueExtractor].getName\n",
    ")\n",
    "\n",
    "/** ********************************\n",
    "Lets write our input dataset to Hudi.\n",
    "************************************/\n",
    "(sourceData.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "\n",
    "  //Operation Key tells Hudi whether this is an Insert, Upsert, or Bulk Insert operation\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, \n",
    "                                   DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL)\n",
    "  \n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9fe45a08cd4550ba18780ee1f5e2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n"
     ]
    }
   ],
   "source": [
    "/** **********************************\n",
    "Querying Hudi data is easy. We set the format to \"org.apache.hudi\"\n",
    "**************************************/\n",
    "val readOptimizedHudiViewDF = (spark.read\n",
    "       .format(\"org.apache.hudi\")\n",
    "       .load(hudiTablePath + \"/*/*/*/*\")\n",
    "       .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e2037f6f6c4569ac5481c761628130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "+-----------+--------+\n",
      "|star_rating|count(1)|\n",
      "+-----------+--------+\n",
      "|          1|   77100|\n",
      "|          2|   38288|\n",
      "|          3|   58836|\n",
      "|          4|  130377|\n",
      "|          5|  599625|\n",
      "|        100|       5|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** ***********************************\n",
    "Lets take a look at our data. Lets say someone says there is something odd going \n",
    "on with star ratings.\n",
    "**************************************/\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_raw_ro_table\");\n",
    "spark.sql(\"\"\"select star_rating, count(*) from amazon_product_reviews_raw_ro_table \n",
    "                                group by star_rating order by star_rating ASC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101b002bada141369359e04deb9d3b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsertdf: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n"
     ]
    }
   ],
   "source": [
    "/** *********************************\n",
    "Select the rows we want to update and and make the update.\n",
    "************************************/\n",
    "val upsertdf = (readOptimizedHudiViewDF.filter($\"star_rating\" === 100)\n",
    "                                .withColumn(\"star_rating\", lit(null).cast(IntegerType)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe9716aef5541c6b8783605051a5515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/** ********************************\n",
    "Before, if you wanted to update data in S3, you had to read the old data, merge with the new data, and then overwrite\n",
    "the old data. Now, with Hudi, you can directly update the data in-place.\n",
    "************************************/\n",
    "(upsertdf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, \n",
    "                    DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4326828ea7874bcda6be5edf7d26ade0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "+-----------+--------+\n",
      "|star_rating|count(1)|\n",
      "+-----------+--------+\n",
      "|       null|       5|\n",
      "|          1|   77100|\n",
      "|          2|   38288|\n",
      "|          3|   58836|\n",
      "|          4|  130377|\n",
      "|          5|  599625|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val readOptimizedHudiViewDF = (spark.read.format(\"org.apache.hudi\")\n",
    "                                    .load(hudiTablePath + \"/*/*/*/*\").cache())\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_ro_table\");\n",
    "\n",
    "spark.sql(\"\"\"select star_rating, count(*) from amazon_product_reviews_ro_table \n",
    "                        group by star_rating order by star_rating ASC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d67c2643b2b408a92c507f58e8f6be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleteRowsDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n"
     ]
    }
   ],
   "source": [
    "/** *******************************\n",
    "Now, suppose we need to delete a customers information due to GDPR because a \n",
    "request was made by a customer?\n",
    "***********************************/\n",
    "val deleteRowsDf = readOptimizedHudiViewDF.filter($\"customer_id\" === 32068341);\n",
    "\n",
    "//Deletion\n",
    "(deleteRowsDf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "  //We set the operation to UPSERT\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, \n",
    "                       DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  //We set the Payload Class to be empty record \n",
    "  .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \n",
    "                       \"org.apache.hudi.EmptyHoodieRecordPayload\")\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b741eea3eae4ce58ec3bd5e4d1b9be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val readOptimizedHudiViewDF = (spark.read.format(\"org.apache.hudi\")\n",
    "                               .load(hudiTablePath + \"/*/*/*/*\").cache())\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_ro_table\");\n",
    "\n",
    "spark.sql(\"\"\"select count(*) from amazon_product_reviews_ro_table where\n",
    "                                    customer_id = 32068341\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58f59285a8c4a1bb118e4d247301a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: Array[String] = Array(20191203042919, 20191203043126)\n",
      "res28: String = [Ljava.lang.String;@372ae764\n"
     ]
    }
   ],
   "source": [
    "/**************************************\n",
    "We can also do point in time queries. Lets take a look at all the commits.\n",
    "***************************************/\n",
    "val commits = (spark.sql(\"\"\"select distinct(_hoodie_commit_time) as commitTime from \n",
    "                            amazon_product_reviews_ro_table order by commitTime\"\"\")\n",
    "                .map(k => k.getString(0)).take(50))\n",
    "commits.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4bfa3c69484b2bb1ab9ea0abcd0335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginTime: String = 0\n",
      "endTime: String = 20191203042919\n",
      "amazon_product_reviews_table: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "+--------------+----------+--------------------+-----------+\n",
      "|     review_id|product_id|       product_title|star_rating|\n",
      "+--------------+----------+--------------------+-----------+\n",
      "|R2LJHK4EC3ZBYZ|B001G0MAVA|Honeywell Premium...|        100|\n",
      "|R39AOJ60FZ1OKE|B008HVH502|Mail Boss 7506 Ma...|        100|\n",
      "|R3Q1OES5YKHF2J|B0042G2W72|Kingston Brass Te...|        100|\n",
      "| RAWOJ1FFIPWGH|B001G0MAZ6|Honeywell RPWL301...|        100|\n",
      "|R21L9NH66PU3HP|B00I0ZGOZM|APEC Top Tier 5-S...|        100|\n",
      "+--------------+----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** ********************************\n",
    "Suppose that we wanted to know what was a review at a certain point of time or a range\n",
    "of commits.\n",
    "************************************/\n",
    "val beginTime = \"0\"\n",
    "val endTime = commits(0) // commit time we are interested in\n",
    "\n",
    "val amazon_product_reviews_table = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     //Mark that we want to do an incremental query\n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, \n",
    "                             DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "\n",
    "     //Set at what time we want to start quering.\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime)\n",
    "     .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)\n",
    "\n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)).cache()\n",
    "\n",
    "(amazon_product_reviews_table.select(\"review_id\", \"product_id\", \"product_title\", \n",
    "                                   \"star_rating\").filter($\"star_rating\" === 100).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf5e6bd096c464fb7306968383a09b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/***********************************\n",
    "Hive and Presto can query the data too!\n",
    "************************************/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
